{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cb78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda =False\n",
    "device = 'cuda' if cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3ddf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'softmaxKmeans' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Sibylse/softmaxKmeans.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b189c532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shess/Scripts/softmaxKmeans\n",
      "remote: Enumerating objects: 303, done.\u001b[K\n",
      "remote: Counting objects: 100% (301/301), done.\u001b[K\n",
      "remote: Compressing objects: 100% (148/148), done.\u001b[K\n",
      "remote: Total 303 (delta 162), reused 287 (delta 153), pack-reused 2\u001b[K\n",
      "Receiving objects: 100% (303/303), 319.73 MiB | 7.95 MiB/s, done.\n",
      "Resolving deltas: 100% (162/162), completed with 2 local objects.\n",
      "From https://github.com/Sibylse/softmaxKmeans\n",
      "   632684c..ffae235  master     -> origin/master\n",
      " * [new branch]      multigpu   -> origin/multigpu\n",
      "Updating 632684c..ffae235\n",
      "Fast-forward\n",
      " checkpoint/Cifar10ResNetGauss.t7                   | Bin \u001b[31m44775307\u001b[m -> \u001b[32m44775307\u001b[m bytes\n",
      " checkpoint/Cifar10_no_gp_ResNetGauss.t7            | Bin \u001b[31m0\u001b[m -> \u001b[32m44775307\u001b[m bytes\n",
      " checkpoint/MNISTLeNetGauss.t7                      | Bin \u001b[31m142084\u001b[m -> \u001b[32m142212\u001b[m bytes\n",
      " checkpoint/MNISTLeNetLinear.t7                     | Bin \u001b[31m142084\u001b[m -> \u001b[32m142212\u001b[m bytes\n",
      " checkpoint/MNISTd2LeNetGauss.t7                    | Bin \u001b[31m0\u001b[m -> \u001b[32m137988\u001b[m bytes\n",
      " checkpoint/MNISTd2LeNetGauss_DUQ.t7                | Bin \u001b[31m0\u001b[m -> \u001b[32m138177\u001b[m bytes\n",
      " checkpoint/MNISTd2LeNetLinear.t7                   | Bin \u001b[31m0\u001b[m -> \u001b[32m137860\u001b[m bytes\n",
      " checkpoint/MNISTd2_baseLeNetGauss.t7               | Bin \u001b[31m0\u001b[m -> \u001b[32m137860\u001b[m bytes\n",
      " checkpoint/MNISTd2_baseLeNetLinear.t7              | Bin \u001b[31m0\u001b[m -> \u001b[32m137860\u001b[m bytes\n",
      " train/layers.py                                    |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " train/loss.py                                      |  53 \u001b[32m++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
      " train/models/__pycache__/__init__.cpython-37.pyc   | Bin \u001b[31m492\u001b[m -> \u001b[32m482\u001b[m bytes\n",
      " train/models/__pycache__/densenet.cpython-37.pyc   | Bin \u001b[31m4138\u001b[m -> \u001b[32m4128\u001b[m bytes\n",
      " train/models/__pycache__/dpn.cpython-37.pyc        | Bin \u001b[31m3558\u001b[m -> \u001b[32m3548\u001b[m bytes\n",
      " train/models/__pycache__/googlenet.cpython-37.pyc  | Bin \u001b[31m2853\u001b[m -> \u001b[32m2843\u001b[m bytes\n",
      " train/models/__pycache__/lenet.cpython-37.pyc      | Bin \u001b[31m1159\u001b[m -> \u001b[32m1560\u001b[m bytes\n",
      " .../models/__pycache__/lenet_gauss.cpython-37.pyc  | Bin \u001b[31m3038\u001b[m -> \u001b[32m3961\u001b[m bytes\n",
      " train/models/__pycache__/mobilenet.cpython-37.pyc  | Bin \u001b[31m2603\u001b[m -> \u001b[32m2593\u001b[m bytes\n",
      " .../models/__pycache__/mobilenetv2.cpython-37.pyc  | Bin \u001b[31m3053\u001b[m -> \u001b[32m3043\u001b[m bytes\n",
      " train/models/__pycache__/pnasnet.cpython-37.pyc    | Bin \u001b[31m4637\u001b[m -> \u001b[32m4627\u001b[m bytes\n",
      " .../__pycache__/preact_resnet.cpython-37.pyc       | Bin \u001b[31m4557\u001b[m -> \u001b[32m4547\u001b[m bytes\n",
      " train/models/__pycache__/resnet.cpython-37.pyc     | Bin \u001b[31m4776\u001b[m -> \u001b[32m5345\u001b[m bytes\n",
      " .../models/__pycache__/resnetGauss.cpython-37.pyc  | Bin \u001b[31m6485\u001b[m -> \u001b[32m6475\u001b[m bytes\n",
      " train/models/__pycache__/resnext.cpython-37.pyc    | Bin \u001b[31m3561\u001b[m -> \u001b[32m3551\u001b[m bytes\n",
      " train/models/__pycache__/senet.cpython-37.pyc      | Bin \u001b[31m3920\u001b[m -> \u001b[32m3910\u001b[m bytes\n",
      " train/models/__pycache__/shufflenet.cpython-37.pyc | Bin \u001b[31m4040\u001b[m -> \u001b[32m4030\u001b[m bytes\n",
      " .../models/__pycache__/shufflenetv2.cpython-37.pyc | Bin \u001b[31m5323\u001b[m -> \u001b[32m5313\u001b[m bytes\n",
      " train/models/__pycache__/vgg.cpython-37.pyc        | Bin \u001b[31m2197\u001b[m -> \u001b[32m2063\u001b[m bytes\n",
      " train/models/__pycache__/vgg_gauss.cpython-37.pyc  | Bin \u001b[31m2977\u001b[m -> \u001b[32m2967\u001b[m bytes\n",
      " 29 files changed, 53 insertions(+), 2 deletions(-)\n",
      " create mode 100644 checkpoint/Cifar10_no_gp_ResNetGauss.t7\n",
      " create mode 100644 checkpoint/MNISTd2LeNetGauss.t7\n",
      " create mode 100644 checkpoint/MNISTd2LeNetGauss_DUQ.t7\n",
      " create mode 100644 checkpoint/MNISTd2LeNetLinear.t7\n",
      " create mode 100644 checkpoint/MNISTd2_baseLeNetGauss.t7\n",
      " create mode 100644 checkpoint/MNISTd2_baseLeNetLinear.t7\n"
     ]
    }
   ],
   "source": [
    "%cd softmaxKmeans\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f794794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "from train.models import *\n",
    "from train import *\n",
    "from train.layers import *\n",
    "from train.loss import *\n",
    "from train.optimization import Optimizer\n",
    "from attack import FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dfd5607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "c=10\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=trans)\n",
    "# Select only some classes for motivating picture\n",
    "idx = testset.targets < c\n",
    "testset.targets = testset.targets[idx]\n",
    "testset.data = testset.data[idx]\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b164e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"checkpoint/\"\n",
    "def load_net(name,dim_embedding,classifier):\n",
    "    net = LeNet(dim_embedding,classifier)\n",
    "    checkpoint = torch.load(path+name+'.t7',map_location='cpu')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    print('LeNet ACC:',checkpoint['acc'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433824b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f49df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet ACC: tensor(99.2600)\n",
      "LeNet ACC: tensor(98.4767)\n",
      "LeNet ACC: tensor(99.3000)\n"
     ]
    }
   ],
   "source": [
    "net_dict={}\n",
    "net_dict[\"sm\"] = load_net('MNISTLeNetLinear',d,nn.Linear(in_features = d, out_features = c))\n",
    "#net_dict[\"sm_gp\"] = load_net('MNISTLeNetLinear_gp',2,nn.Linear(in_features = 2, out_features = c))\n",
    "net_dict[\"duq\"] = load_net('MNISTLeNetGauss_DUQ',d,Gauss_DUQ(in_features = d, out_features = c, gamma=10))\n",
    "net_dict[\"ga\"] = load_net('MNISTLeNetGauss',d,Gauss(in_features = d, out_features = c, gamma=10))\n",
    "#net_dict[\"ga_mv\"] = load_net('MNISTLeNetGauss_MV',2,Gauss_MV(in_features = 2, out_features = c, gamma=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fdea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_dict = {}\n",
    "criterion_dict[\"sm\"] = CE_Loss(net_dict[\"sm\"].classifier, c, device)\n",
    "#criterion_dict[\"sm_gp\"] = CE_Loss(net_dict[\"sm_gp\"].classifier, c, device)\n",
    "criterion_dict[\"ga\"] = CE_GALoss(net_dict[\"ga\"].classifier, c, device)\n",
    "#criterion_dict[\"ga_mv\"] = BCE_GALoss(net_dict[\"ga_mv\"].classifier, c, device)\n",
    "criterion_dict[\"duq\"] = BCE_DUQLoss(net_dict[\"duq\"].classifier, c, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7797639",
   "metadata": {},
   "source": [
    "# Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0874a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/BorealisAI/advertorch.git\n",
      "  Cloning https://github.com/BorealisAI/advertorch.git to /private/var/folders/3h/v9rwrxsj02n4bvnfdw3s81vc0000gn/T/pip-req-build-x3cdepzr\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/BorealisAI/advertorch.git /private/var/folders/3h/v9rwrxsj02n4bvnfdw3s81vc0000gn/T/pip-req-build-x3cdepzr\n",
      "  Resolved https://github.com/BorealisAI/advertorch.git to commit e063e341c87d9a621ae1a8f72c3507d5ea5bd327\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/BorealisAI/advertorch.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d4bc8",
   "metadata": {},
   "source": [
    "## Carlini & Wagner Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "171ff039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from advertorch.attacks import CarliniWagnerL2Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42962937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sm', 'duq', 'ga'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e431baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6b1a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE_Loss tensor(402)\n",
      "BCE_DUQLoss tensor(41)\n",
      "CE_GALoss tensor(1)\n"
     ]
    }
   ],
   "source": [
    "cw_dict, examples_dict={},{}\n",
    "for net_key in net_dict.keys():\n",
    "    cw = CarliniWagnerL2Attack(net_dict[net_key], c, confidence=-np.log(0.2), max_iterations=100, clip_min=-0.5, clip_max=0.5 )\n",
    "    examples_dict[net_key]= cw.perturb(inputs, targets)\n",
    "    criterion = criterion_dict[net_key]\n",
    "    conf,pred = criterion.conf(net_dict[net_key].embed(examples_dict[net_key])).max(1)\n",
    "    print(criterion.__class__.__name__,((targets != pred) & (conf>0.1)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ea368",
   "metadata": {},
   "source": [
    "# One Pixel Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3085a9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE_Loss tensor(14)\n",
      "BCE_DUQLoss tensor(2)\n",
      "CE_GALoss tensor(1)\n"
     ]
    }
   ],
   "source": [
    "from advertorch.attacks import SinglePixelAttack\n",
    "cw_dict, examples_dict={},{}\n",
    "for net_key in net_dict.keys():\n",
    "    criterion = criterion_dict[net_key]\n",
    "    attack = SinglePixelAttack(net_dict[net_key].embed, max_pixels=1, loss_fn=criterion, clip_min=-0.5, clip_max=0.5)\n",
    "    examples_dict[net_key]= attack.perturb(inputs, targets)\n",
    "    conf,pred = criterion.conf(net_dict[net_key].embed(examples_dict[net_key])).max(1)\n",
    "    print(criterion.__class__.__name__,((targets != pred) & (conf>0.1)).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee70260",
   "metadata": {},
   "source": [
    "# FGSM attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60dba228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE_Loss tensor(130)\n",
      "BCE_DUQLoss tensor(41)\n",
      "CE_GALoss tensor(29)\n"
     ]
    }
   ],
   "source": [
    "from advertorch.attacks import GradientSignAttack\n",
    "cw_dict, examples_dict={},{}\n",
    "for net_key in net_dict.keys():\n",
    "    criterion = criterion_dict[net_key]\n",
    "    attack = GradientSignAttack(net_dict[net_key].embed, eps=0.1, loss_fn=criterion, clip_min=-0.5, clip_max=0.5)\n",
    "    examples_dict[net_key]= attack.perturb(inputs, targets)\n",
    "    conf,pred = criterion.conf(net_dict[net_key].embed(examples_dict[net_key])).max(1)\n",
    "    print(criterion.__class__.__name__,((targets != pred) & (conf>=0.1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "139f084a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_dict[\"ga\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a0b9f",
   "metadata": {},
   "source": [
    "# Linf PGD Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc37b920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE_Loss tensor(24)\n",
      "BCE_DUQLoss tensor(45)\n",
      "CE_GALoss tensor(14)\n"
     ]
    }
   ],
   "source": [
    "from advertorch.attacks import LinfPGDAttack\n",
    "cw_dict, examples_dict={},{}\n",
    "for net_key in net_dict.keys():\n",
    "    criterion = criterion_dict[net_key]\n",
    "    attack = LinfPGDAttack(net_dict[net_key].embed, loss_fn=criterion, eps=0.1, nb_iter=40, eps_iter=0.01, clip_min=-0.5, clip_max=0.5)\n",
    "    examples_dict[net_key]= attack.perturb(inputs, targets)\n",
    "    conf,pred = criterion.conf(net_dict[net_key].embed(examples_dict[net_key])).max(1)\n",
    "    print(criterion.__class__.__name__,((targets != pred) & (conf>=0.1)).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd57a7",
   "metadata": {},
   "source": [
    "# L2PGDAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c73d147e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L2PGDAttack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m net_key \u001b[38;5;129;01min\u001b[39;00m net_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      4\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m criterion_dict[net_key]\n\u001b[0;32m----> 5\u001b[0m     attack \u001b[38;5;241m=\u001b[39m \u001b[43mL2PGDAttack\u001b[49m(net_dict[net_key]\u001b[38;5;241m.\u001b[39membed, loss_fn\u001b[38;5;241m=\u001b[39mcriterion, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, nb_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, eps_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, clip_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m, clip_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      6\u001b[0m     examples_dict[net_key]\u001b[38;5;241m=\u001b[39m attack\u001b[38;5;241m.\u001b[39mperturb(inputs, targets)\n\u001b[1;32m      7\u001b[0m     conf,pred \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mconf(net_dict[net_key]\u001b[38;5;241m.\u001b[39membed(examples_dict[net_key]))\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L2PGDAttack' is not defined"
     ]
    }
   ],
   "source": [
    "from advertorch.attacks import L2PGDAttack\n",
    "cw_dict, examples_dict={},{}\n",
    "for net_key in net_dict.keys():\n",
    "    criterion = criterion_dict[net_key]\n",
    "    attack = L2PGDAttack(net_dict[net_key].embed, loss_fn=criterion, eps=0.4, nb_iter=40, eps_iter=0.01, clip_min=-0.5, clip_max=0.5)\n",
    "    examples_dict[net_key]= attack.perturb(inputs, targets)\n",
    "    conf,pred = criterion.conf(net_dict[net_key].embed(examples_dict[net_key])).max(1)\n",
    "    print(criterion.__class__.__name__,((targets != pred) & (conf>0.1)).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "428107dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE_Loss tensor(1)\n",
      "CE_Loss tensor(7)\n",
      "BCE_DUQLoss tensor(2)\n",
      "BCE_GALoss tensor(0)\n",
      "BCE_GALoss tensor(0)\n"
     ]
    }
   ],
   "source": [
    "cw_dict, examples_dict={},{}\n",
    "for net_key in net_dict.keys():\n",
    "    criterion = criterion_dict[net_key]\n",
    "    attack = L2PGDAttack(net_dict[net_key].embed, loss_fn=criterion, eps=0.4, nb_iter=40, eps_iter=0.01, clip_min=-0.5, clip_max=0.5)\n",
    "    examples_dict[net_key]= attack.perturb(inputs, targets)\n",
    "    conf,pred = criterion.conf(net_dict[net_key].embed(examples_dict[net_key])).max(1)\n",
    "    print(criterion.__class__.__name__,((targets != pred) & (conf>0.1)).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c481bca",
   "metadata": {},
   "source": [
    "# Sparse L1 Descent Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfa3a889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm\n",
      "CE_Loss tensor(0)\n",
      "duq\n",
      "BCE_DUQLoss tensor(0)\n",
      "ga\n",
      "CE_GALoss tensor(0)\n"
     ]
    }
   ],
   "source": [
    "from advertorch.attacks import SparseL1DescentAttack\n",
    "adv_dict, examples_dict={},{}\n",
    "for net_key in net_dict.keys():\n",
    "    print(net_key)\n",
    "    criterion = criterion_dict[net_key]\n",
    "    adversary = SparseL1DescentAttack(predict=net_dict[net_key].embed, eps=0.3, loss_fn=criterion, clip_min=-0.5, clip_max=0.5)\n",
    "    examples_dict[net_key]= adversary.perturb(inputs, targets)\n",
    "    conf,pred = criterion.conf(net_dict[net_key].embed(examples_dict[net_key])).max(1)\n",
    "    print(criterion.__class__.__name__,((targets != pred) & (conf>0.1)).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73517eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output = next(iter(testloader))\n",
    "torch.min(input), torch.max(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66b764",
   "metadata": {},
   "source": [
    "# Plot examples\n",
    "Doesn't work as is, is still adapted to the epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "def plot_mnist_adv(net, criterion, examples, min_conf=0.1):\n",
    "    plt.figure(figsize=(8,15))\n",
    "\n",
    "    conf_pert, pred_pert = criterion.conf(net.embed(examples)).max(1)\n",
    "    idx = (conf_pert>min_conf)\n",
    "    examples, conf_pert, pred_pert = examples[idx], conf_pert[idx], pred_pert[idx]\n",
    "    for j in range(min(len(examples),8)):\n",
    "        plt.subplot(1,8,j+1)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        plt.title(\"{} ({:.3f})\".format(pred_pert[j], conf_pert[j]))\n",
    "        plt.imshow(examples[j].squeeze().detach().numpy(), cmap=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_key = \"ga\"\n",
    "plot_mnist_adv(net_dict[net_key],criterion_dict[net_key],examples_dict[net_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51642a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
